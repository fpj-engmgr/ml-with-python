
Stochastic Gradient Descent

- minimize cost one sample at a time
- approximation methodology
- iterate to get closer
- loss may increase if an outlier is used (PROBLEM)


In PyTorch:

w = torch.tensor(-15.0, requires_grad = True)
b = torch.tensor(-10.0, requires_grad = True)
#
X = torch.arange(-3, 3, 0.1).view(-1, 1)
f = 3 * X
#
def forward(x):
    y = w*x + b
    return y
#
def criterion(yhat, y):
    return torch.mean((yhat - y) ** 2)
#
lr = 0.1
LOSS = []
COST = []
for epoch in range(4):
    total = 0
    for x,y in zip(X,Y):
        yhat = forward(x)
        loss = criterion(yhat, y)
        loss.backward()
        w.data = w.data - lr * w.grad.data
        b.data = b.data - lr * b.grad.data
        w.grad.data.zero_()
        b.grad.data.zero_()
        LOSS.append(loss.item())
        total += loss.item()
    COST.append(total)

# Using dataloader
from torch.utils.data import Dataset

class Data(Dataset):
    def __init__(self):

        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)
        self.y = -3*X + 1
        self.len = self.x.shape[0]
    
    def __getitem__(self, index):
        return self.x[index],self.y[index]
    
    def __len__(self):
        return self.len

dataset=Data()
#
trainloader = DataLoader(dataset=dataset,batch_size=1)
