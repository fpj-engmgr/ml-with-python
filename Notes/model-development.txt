Model:
- A mathematical equiation used to predict a value given one or more values 

- Simple Linear Regression
- Multiple Linear Regression
- Polynomial Regression

Linear Regression
- Single (Simple) or multiple (Multiple) independent variables to predict

Process:
1. Import a linear model from scikit-learn
- from scikit.linear_model import LinearRegression
2. Create a linear regression object using the constructor:
- lm=LinearRegression()
3. define the predictor variable and the target variableL
- X = df[['highway-mpg']]
- Y = df['price']
4. Use lm.fit(X, Y) to fit the model
5. Get a prediction:
- Yhat=lm.predict(X)
Note: the intercept (lm.intercept_) and slope (lm.coeff_) are attributes of lm

Multiple Linear Regression (MLR)

Process:
1. Extract predictor variables:
- Z = df[['horsepower', 'curb-wieght', 'engine-size', 'highway-mpg']]
2. Train the model:
- lm.fit(Z, df['price'])
#. Get a prediction:
- Yhat=lm.predict(X)

Visualization:

Regression plot:

import seaborn as sns 
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

To check for fit type a residual plot is useful:

sns.residplot(df=['highway-mpg'], df['price'])

Distribution plots are useful for understanding slope of multiple independent variables

ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(Yhat, hist=False, color="b", label="Fitted Values", ax=ax1)


Polynomial Regression

Process:

1. Calculate Polynomial of 3rd order
  - f = np.polyfit(x, y, 3)
  - p = np.poly1d(f)
2. print(p)

For multidimensional polynomial regression

from sklearn.preprocessing import PolynomialFeatures
pr = PolynomialFeatures(degree=2, include_bias=False)
x_polly=pr.fit_transfrom(x[['horsepower', 'curb-weight']])

Normalize each feature simultaneouslyL

from sklearn.preprocessing import StandardScaler
SCALE=StandardScaler()
SCALE.fit(x_data[['horsepower', 'highway-mpg']])

x_scale=SCALE.transform(x_data[['horsepower', 'highway-mpg']])

Pipelines:
- sequentially performs transformations

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

Input=[('polynomial', PolynomialFeatures(degree=2)), ('scale', StandardScaler()), ..., ('Model', LinearRegression())]

Pipe=Pipeline(Input)
# train the pipeline object
Pipe.fit(df[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat=Pipe.predict(X[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])

Mean Squared Error
from sklearn.metrics import mean_squared_error
mean_squared_error(df['price'], Y_predict_simple_fit)

R-squared (aka coefficient of determination)
R^2 = 1 - (MSE of Regression Line)/(MSE of average of datapoints)

lm.score(X, y) get the R^2

Prediction and Decision Making

Do the predicted values make sense?

import numpy as np
# generate a sequence from 1->100
new_input=np.arange(1, 101, 1).reshape(-1, 1)
yhat=lm.predict(new_input)

